# -*- coding: utf-8 -*-
"""7018.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zF7ljjhScfTUf3wrWur30U4TW3qnXsa9
"""

import numpy as np
from sklearn.preprocessing import MinMaxScaler
import os
import sys
sys.path.insert(0, '.')
here = os.path.dirname(os.path.abspath(__file__))

# 加载数据集
data_path = os.path.join(here, "PEMS03_num31.npz")
data = np.load(data_path)['data']

# 数据预处理
scaler = MinMaxScaler(feature_range=(0, 1))
data_normalized = scaler.fit_transform(data.reshape(-1, 1)).reshape(-1)

# 定义函数，将序列数据转换为可以用于训练LSTM的格式
def create_dataset(data, look_back=7):
    X, y = [], []
    for i in range(len(data)-look_back):
        X.append(data[i:i+look_back])
        y.append(data[i+look_back])
    return np.array(X), np.array(y)

# 使用滑动窗口创建训练集、验证集和测试集
look_back = 7
train_size = int(len(data_normalized) * 0.7)
valid_size = int(len(data_normalized) * 0.2)
test_size = len(data_normalized) - train_size - valid_size

train_data = data_normalized[:train_size]
valid_data = data_normalized[train_size:train_size+valid_size]
test_data = data_normalized[train_size+valid_size:]

train_X, train_y = create_dataset(train_data, look_back)
valid_X, valid_y = create_dataset(valid_data, look_back)
test_X, test_y = create_dataset(test_data, look_back)

import tensorflow as tf

# 构建LSTM模型
model = tf.keras.Sequential([
    tf.keras.layers.LSTM(64, input_shape=(look_back, 1),return_sequences=True),
    tf.keras.layers.LSTM(32),
    tf.keras.layers.Dense(50),
    tf.keras.layers.Dense(1)
])

# 编译模型
model.compile(optimizer='adam', loss='mean_squared_error')

# 训练模型
history = model.fit(train_X, train_y, epochs=100, batch_size=64, validation_data=(valid_X, valid_y))

# 评估模型
loss = model.evaluate(test_X, test_y)
print("Test Loss:", loss)

# 保存模型
model.save('lstm_model.h5')

import tensorflow as tf

# 构建CNN-LSTM模型
model = tf.keras.Sequential([
    tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(look_back, 1)),
    tf.keras.layers.MaxPooling1D(pool_size=2),
    tf.keras.layers.LSTM(64, return_sequences=True),
    tf.keras.layers.LSTM(32),
    tf.keras.layers.Dense(50),
    tf.keras.layers.Dense(1)
])

# 编译模型
model.compile(optimizer='adam', loss='mean_squared_error')

# 训练模型
history = model.fit(train_X, train_y, epochs=100, batch_size=64, validation_data=(valid_X, valid_y))

# 评估模型
loss = model.evaluate(test_X, test_y)
print("Test Loss:", loss)

# 保存模型
model.save('cnn_lstm_model.h5')

import tensorflow as tf

# 构建简单的RNN模型
model = tf.keras.Sequential([
    tf.keras.layers.SimpleRNN(64, input_shape=(look_back, 1), return_sequences=True),
    tf.keras.layers.SimpleRNN(32),
    tf.keras.layers.Dense(50),
    tf.keras.layers.Dense(1)
])

# 编译模型
model.compile(optimizer='adam', loss='mean_squared_error')

# 训练模型
history = model.fit(train_X, train_y, epochs=100, batch_size=64, validation_data=(valid_X, valid_y))

# 评估模型
loss = model.evaluate(test_X, test_y)
print("Test Loss:", loss)

# 保存模型
model.save('rnn_model.h5')



import tensorflow as tf

# 定义注意力机制层
class AttentionLayer(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        self.W = self.add_weight(name="att_weight", shape=(input_shape[-1], 1), initializer="normal")
        self.b = self.add_weight(name="att_bias", shape=(input_shape[1], 1), initializer="zeros")
        super(AttentionLayer, self).build(input_shape)

    def call(self, x):
        et = tf.keras.backend.dot(x, self.W)
        et = tf.keras.activations.tanh(et + self.b)
        at = tf.keras.activations.softmax(et, axis=1)
        output = x * at
        return tf.keras.backend.sum(output, axis=1)

# 构建带有注意力机制的 LSTM 模型
model = tf.keras.Sequential([
    tf.keras.layers.LSTM(64, input_shape=(look_back, 1), return_sequences=True),
    AttentionLayer(),  # 添加注意力机制层
    tf.keras.layers.Dense(50),
    tf.keras.layers.Dense(1)
])

# 编译模型
model.compile(optimizer='adam', loss='mean_squared_error')

# 训练模型
history = model.fit(train_X, train_y, epochs=100, batch_size=64, validation_data=(valid_X, valid_y))

# 评估模型
loss = model.evaluate(test_X, test_y)
print("Test Loss:", loss)

# 保存模型
model.save('lstm_model_with_attention.h5')



# 定义函数，将序列数据转换为可以用于训练CNN的格式
def create_dataset(data, look_back=7):
    X, y = [], []
    for i in range(len(data)-look_back):
        X.append(data[i:i+look_back])
        y.append(data[i+look_back])
    return np.array(X), np.array(y)

# 使用滑动窗口创建训练集、验证集和测试集
look_back = 7
train_size = int(len(data_normalized) * 0.7)
valid_size = int(len(data_normalized) * 0.2)
test_size = len(data_normalized) - train_size - valid_size

train_data = data_normalized[:train_size]
valid_data = data_normalized[train_size:train_size+valid_size]
test_data = data_normalized[train_size+valid_size:]

train_X, train_y = create_dataset(train_data, look_back)
valid_X, valid_y = create_dataset(valid_data, look_back)
test_X, test_y = create_dataset(test_data, look_back)

# 调整输入数据的形状以适应CNN
train_X = train_X.reshape(train_X.shape[0], train_X.shape[1], 1)
valid_X = valid_X.reshape(valid_X.shape[0], valid_X.shape[1], 1)
test_X = test_X.reshape(test_X.shape[0], test_X.shape[1], 1)

# 构建CNN模型
model = tf.keras.Sequential([
    tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(look_back, 1)),
    tf.keras.layers.MaxPooling1D(pool_size=2),
    # tf.keras.layers.Dropout(0.5),  # 添加dropout层
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(100),
    tf.keras.layers.Dense(1)
])

# 编译模型
model.compile(optimizer='adam', loss='mean_squared_error')

# 训练模型
history = model.fit(train_X, train_y, epochs=50, batch_size=64, validation_data=(valid_X, valid_y))

# 评估模型
loss = model.evaluate(test_X, test_y)
print("Test Loss:", loss)

# 保存模型
model.save('cnn_model.h5')



import matplotlib.pyplot as plt

# 使用模型进行预测
train_predict = model.predict(train_X)
valid_predict = model.predict(valid_X)
test_predict = model.predict(test_X)

# 反归一化预测值和真实值
train_predict = scaler.inverse_transform(train_predict)
train_y = scaler.inverse_transform(train_y.reshape(-1, 1))
valid_predict = scaler.inverse_transform(valid_predict)
valid_y = scaler.inverse_transform(valid_y.reshape(-1, 1))
test_predict = scaler.inverse_transform(test_predict)
test_y = scaler.inverse_transform(test_y.reshape(-1, 1))

# 绘制测试集预测结果对比图
plt.figure(figsize=(14, 7))
plt.plot(test_y, label='True Values')
plt.plot(test_predict, label='Predictions')
plt.title('CNN: True Values vs Predictions')
plt.xlabel('Time')
plt.ylabel('Traffic Flow')
plt.legend()
plt.show()







# 定义函数，将序列数据转换为可以用于训练CNN的格式
def create_dataset(data, look_back=7):
    X, y = [], []
    for i in range(len(data)-look_back):
        X.append(data[i:i+look_back])
        y.append(data[i+look_back])
    return np.array(X), np.array(y)

# 使用滑动窗口创建训练集和测试集
look_back = 7
train_size = int(len(data_normalized) * 0.9)
train_data = data_normalized[:train_size]
test_data = data_normalized[train_size:]

train_X, train_y = create_dataset(train_data, look_back)
test_X, test_y = create_dataset(test_data, look_back)

# 调整输入数据的形状以适应CNN
train_X = train_X.reshape(train_X.shape[0], train_X.shape[1], 1)
test_X = test_X.reshape(test_X.shape[0], test_X.shape[1], 1)

# 构建CNN模型
model = tf.keras.Sequential([
    tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(look_back, 1)),
    tf.keras.layers.MaxPooling1D(pool_size=2),
    # tf.keras.layers.Dropout(0.5),  # 添加dropout层
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(100),
    tf.keras.layers.Dense(1)
])

# 编译模型
model.compile(optimizer='adam', loss='mean_squared_error')

# 训练模型
history = model.fit(train_X, train_y, epochs=50, batch_size=64)

# 评估模型
loss = model.evaluate(test_X, test_y)
print("Test Loss:", loss)

# 保存模型
model.save('cnn_model.h5')



# 定义函数，将序列数据转换为可以用于训练CNN的格式
def create_dataset(data, look_back=7):
    X, y = [], []
    for i in range(len(data)-look_back):
        X.append(data[i:i+look_back])
        y.append(data[i+look_back])
    return np.array(X), np.array(y)

# 使用滑动窗口创建训练集和测试集
look_back = 7
train_size = int(len(data_normalized) * 0.9)
train_data = data_normalized[:train_size]
test_data = data_normalized[train_size:]

train_X, train_y = create_dataset(train_data, look_back)
test_X, test_y = create_dataset(test_data, look_back)

import tensorflow as tf

# 构建LSTM模型
model = tf.keras.Sequential([
    tf.keras.layers.LSTM(64, input_shape=(look_back, 1),return_sequences=True),
    tf.keras.layers.LSTM(32),
    tf.keras.layers.Dense(1)
])

# 编译模型
model.compile(optimizer='adam', loss='mean_squared_error')

# 训练模型
history = model.fit(train_X, train_y, epochs=100, batch_size=64)

# 评估模型
loss = model.evaluate(test_X, test_y)
print("Test Loss:", loss)

# 保存模型
model.save('lstm_model.h5')



import matplotlib.pyplot as plt

# 使用模型进行预测
train_predict = model.predict(train_X)
test_predict = model.predict(test_X)

# 反归一化预测值和真实值
train_predict = scaler.inverse_transform(train_predict)
train_y = scaler.inverse_transform(train_y.reshape(-1, 1))
test_predict = scaler.inverse_transform(test_predict)
test_y = scaler.inverse_transform(test_y.reshape(-1, 1))

# 绘制测试集预测结果对比图
plt.figure(figsize=(14, 7))
plt.plot(test_y, label='True Values')
plt.plot(test_predict, label='Predictions')
plt.title('CNN: True Values vs Predictions')
plt.xlabel('Time')
plt.ylabel('Traffic Flow')
plt.legend()
plt.show()