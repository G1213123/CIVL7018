{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f36c1a31",
   "metadata": {},
   "source": [
    "## Import packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3fcbbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def compute_metric(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mape = mean_absolute_percentage_error(y_true[np.where(y_true > 5)[0]], y_pred[np.where(y_true > 5)[0]])\n",
    "    return mae, rmse, mape\n",
    "  \n",
    "def get_dataloader(X, y, device, bs, shuffle):\n",
    "    return torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.FloatTensor(X).to(device), \n",
    "                    torch.FloatTensor(y).to(device)), batch_size=bs, shuffle=shuffle, drop_last=False)\n",
    "\n",
    "def calculate_metric_torch(true, pred, mask_value=5):\n",
    "    mae = torch.mean(torch.abs(true - pred))\n",
    "    rmse = torch.sqrt(torch.mean((pred - true) ** 2))\n",
    "    if mask_value != None:\n",
    "        mask = torch.gt(true, mask_value)\n",
    "        pred = torch.masked_select(pred, mask)\n",
    "        true = torch.masked_select(true, mask)\n",
    "    mape = torch.mean(torch.abs(torch.div((true - pred), true)))\n",
    "    return mae, rmse, mape\n",
    "\n",
    "def trainer(model, lr, epochs, train_loader, test_loader):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    MSE = nn.MSELoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = list()\n",
    "        for src, trg in train_loader:\n",
    "            pred = model(src)\n",
    "            loss = MSE(pred, trg)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            epoch_loss.append(loss.item())\n",
    "        epoch_loss = np.mean(epoch_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            preds, trues = list(), list()\n",
    "            for src, trg in test_loader:\n",
    "                pred = model(src)\n",
    "                preds.append(pred)\n",
    "                trues.append(trg)\n",
    "            \n",
    "            mae, rmse, mape = calculate_metric_torch(torch.cat(trues), torch.cat(preds))\n",
    "        \n",
    "        print('Epoch %d, training loss: %.3f, test mae: %.3f, rmse: %.3f, mape: %.3f' % (epoch, epoch_loss, mae, rmse, mape))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d39cd5f",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e71a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = np.load('./processed_demand_datasetsMAN.npz')\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = datasets['trainX'], datasets['valX'], datasets['testX'], datasets['trainy'], datasets['valy'], datasets['testy']\n",
    "\n",
    "\n",
    "num_nodes = X_train.shape[0]\n",
    "edges = np.load('./edges_GAman.npy')\n",
    "\n",
    "adj = np.zeros((num_nodes, num_nodes))\n",
    "\n",
    "for i in range(num_nodes): \n",
    "    adj[i,i] = 1\n",
    "    \n",
    "for i in range(len(edges)):\n",
    "    adj[edges[i,0], edges[i,1]] = 1\n",
    "    \n",
    "adj = torch.LongTensor(adj)\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c5dc979",
   "metadata": {},
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afb05e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test mae: 2.164, rmse: 4.394, mape: 21.693\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(random_state=1000)\n",
    "rf.fit(X_train.reshape(-1,42), y_train.reshape(-1))\n",
    "\n",
    "pred = rf.predict(X_test.reshape(-1,42))\n",
    "mae, rmse, mape = compute_metric(y_test.reshape(-1), pred)\n",
    "\n",
    "print('test mae: %.3f, rmse: %.3f, mape: %.3f' % (mae, rmse, mape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1427b2a7",
   "metadata": {},
   "source": [
    "## GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19ccb09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test mae: 2.224, rmse: 4.432, mape: 22.182\n"
     ]
    }
   ],
   "source": [
    "gbdt = GradientBoostingRegressor(random_state=1000)\n",
    "gbdt.fit(X_train.reshape(-1,42), y_train.reshape(-1))\n",
    "\n",
    "pred = gbdt.predict(X_test.reshape(-1,42))\n",
    "mae, rmse, mape = compute_metric(y_test.reshape(-1), pred)\n",
    "print('test mae: %.3f, rmse: %.3f, mape: %.3f' % (mae, rmse, mape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7bef049d",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61100aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test mae: 2.164, rmse: 4.394, mape: 21.577\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPRegressor(random_state=200, max_iter=400)\n",
    "mlp.fit(X_train.reshape(-1,42), y_train.reshape(-1))\n",
    "\n",
    "pred = mlp.predict(X_test.reshape(-1,42))\n",
    "mae, rmse, mape = compute_metric(y_test.reshape(-1), pred)\n",
    "print('test mae: %.3f, rmse: %.3f, mape: %.3f' % (mae, rmse, mape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58252853",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74edb7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, hidden_dim1, num_layers, hidden_dim2):\n",
    "        super().__init__()\n",
    "        self.GRU = nn.GRU(1, hidden_dim1, num_layers, batch_first=True)\n",
    "        self.Dense = nn.Sequential(nn.Linear(hidden_dim1, hidden_dim2), nn.ReLU(), nn.Linear(hidden_dim2, 1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        o, h = self.GRU(x)\n",
    "        x = self.Dense(h[-1])\n",
    "        return x.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6fe72db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training loss: 136.192, test mae: 2.329, rmse: 4.850, mape: 0.245\n",
      "Epoch 1, training loss: 26.016, test mae: 2.373, rmse: 4.814, mape: 0.222\n",
      "Epoch 2, training loss: 25.260, test mae: 2.212, rmse: 4.512, mape: 0.213\n",
      "Epoch 3, training loss: 24.239, test mae: 2.188, rmse: 4.479, mape: 0.222\n",
      "Epoch 4, training loss: 23.993, test mae: 2.345, rmse: 4.716, mape: 0.249\n",
      "Epoch 5, training loss: 23.935, test mae: 2.261, rmse: 4.513, mape: 0.226\n",
      "Epoch 6, training loss: 23.546, test mae: 2.453, rmse: 5.274, mape: 0.227\n",
      "Epoch 7, training loss: 23.321, test mae: 2.202, rmse: 4.500, mape: 0.225\n",
      "Epoch 8, training loss: 23.199, test mae: 2.224, rmse: 4.465, mape: 0.213\n",
      "Epoch 9, training loss: 23.150, test mae: 2.305, rmse: 4.442, mape: 0.209\n"
     ]
    }
   ],
   "source": [
    "train_loader = get_dataloader(X_train.reshape(-1,42,1), y_train.reshape(-1), device, 64, True)\n",
    "test_loader = get_dataloader(X_test.reshape(-1,42,1), y_test.reshape(-1), device, 64, False)\n",
    "\n",
    "#####################################################################################################################\n",
    "# After class, you can try different learning rates and training epochs yourself. \n",
    "# Here for quick results, we just set the training epoch to 10.\n",
    "lr = 0.001\n",
    "epochs = 10\n",
    "# For the sake of simplicity, we also omitted the parameter tuning process here. \n",
    "# The correct way is to adjust on the validation set to get the best hyperparameters, and then evaluate the modelbased on the test set.\n",
    "#####################################################################################################################\n",
    "\n",
    "model = GRU(64, 2, 32).to(device)\n",
    "\n",
    "trainer(model, lr, epochs, train_loader, test_loader)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2aa1e5c7",
   "metadata": {},
   "source": [
    "## GRU-GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, device, in_dim, out_dim, alpha=0.2, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.alpha = alpha\n",
    "        self.dropout = dropout\n",
    "        self.weights = nn.Parameter(torch.FloatTensor(in_dim, out_dim))\n",
    "        nn.init.xavier_normal_(self.weights.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.FloatTensor(2*out_dim, 1))\n",
    "        nn.init.xavier_normal_(self.a.data, gain=1.414)\n",
    "\n",
    "    def forward(self, h, adj):\n",
    "        \"\"\"\n",
    "        h: (bs, num_node, in_dim)\n",
    "        adj: (num_node, num_node)\n",
    "        return (bs, num_node, out_dim)\n",
    "        \"\"\"\n",
    "        bs, num_node, in_dim = h.size()\n",
    "        src, trg = torch.nonzero(adj.long(), as_tuple=True)\n",
    "        \n",
    "        Wh = torch.matmul(h, self.weights) # (bs, num_node, out_dim)\n",
    "        edge_h = torch.cat([Wh[:,src,:], Wh[:,trg,:]], dim=-1) # (bs, num_edges, 2*out_dim)\n",
    "        edge_e = F.leaky_relu(torch.matmul(edge_h, self.a), negative_slope=self.alpha).squeeze(-1) # (bs, num_edges)\n",
    "\n",
    "        attention = -9e15*torch.ones(bs, num_node, num_node).to(self.device) #（bs, num_node, num_node)\n",
    "        attention[:, src, trg] = edge_e\n",
    "        attention = F.dropout(F.softmax(attention, dim=-1), self.dropout) #（bs, num_node, num_node)\n",
    "\n",
    "        h_prime = torch.einsum('bij,bjo->bio', attention, Wh)\n",
    "\n",
    "        return h_prime\n",
    "    \n",
    "    \n",
    "class GRUGAT(nn.Module):\n",
    "    def __init__(self, device, in_dim, out_dim, num_node, adj):\n",
    "        super().__init__()\n",
    "        self.adj = adj\n",
    "        self.gru = nn.GRU(1, in_dim, 2, batch_first=True)\n",
    "        self.gat = GATLayer(device, in_dim, out_dim)\n",
    "        self.end_conv = nn.Conv1d(out_dim, 1, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (bs, num_node, time_step)\n",
    "        X = []\n",
    "        for i in range(x.size(1)):\n",
    "            o, h = self.gru(x[:,i,:].unsqueeze(-1))\n",
    "            X.append(h[-1])\n",
    "        X = torch.stack(X)\n",
    "        X = self.gat(X.permute(1,0,2), self.adj) # (bs, num_node, 16)\n",
    "        X = self.end_conv(X.permute(0,2,1))\n",
    "        return X.squeeze(1)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a769b89d",
   "metadata": {},
   "source": [
    "For a complex model, such as the hybrid model of GAT and GRU used here, the training time is longer due to the larger number of parameters involved. The hyperparameter tuning process of complex models is usually more complicated, and some training tricks need to be involved, such as learning rate decay and early stopping mechanism. \\\n",
    "Here, also for simplicity and quick display of results, we only set the training epoch to 10 and set the learning rate to a constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f415df23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training loss: 710.472, test mae: 16.027, rmse: 23.200, mape: 0.630\n",
      "Epoch 1, training loss: 650.352, test mae: 12.684, rmse: 22.100, mape: 0.643\n",
      "Epoch 2, training loss: 516.253, test mae: 10.718, rmse: 19.025, mape: 0.703\n",
      "Epoch 3, training loss: 469.101, test mae: 10.864, rmse: 19.237, mape: 0.629\n",
      "Epoch 4, training loss: 464.243, test mae: 10.738, rmse: 19.038, mape: 0.654\n",
      "Epoch 5, training loss: 458.157, test mae: 10.460, rmse: 18.958, mape: 0.652\n",
      "Epoch 6, training loss: 449.544, test mae: 10.826, rmse: 19.473, mape: 0.650\n",
      "Epoch 7, training loss: 454.530, test mae: 10.591, rmse: 18.845, mape: 0.638\n",
      "Epoch 8, training loss: 454.973, test mae: 10.363, rmse: 18.585, mape: 0.682\n",
      "Epoch 9, training loss: 448.944, test mae: 10.341, rmse: 18.524, mape: 0.651\n"
     ]
    }
   ],
   "source": [
    "train_loader = get_dataloader(X_train.transpose(1,0,2), y_train.T, device, 36, True)\n",
    "test_loader = get_dataloader(X_test.transpose(1,0,2), y_test.T, device, 36, False)\n",
    "\n",
    "lr = 0.01\n",
    "epochs = 10\n",
    "\n",
    "model = GRUGAT(device, 10, 32, 198, adj).to(device)\n",
    "\n",
    "trainer(model, lr, epochs, train_loader, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2512d956",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9bdd6def",
   "metadata": {},
   "source": [
    "## Use Colab to reproduce the above code on the cloud"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24718656",
   "metadata": {},
   "source": [
    "If you have not configured the Pytorch environment (GPU version) on your personal computer, you may not be able to run this notebook \n",
    "or it will run very slowly. \\\n",
    "In this case, you might consider using Colab, a free-to-use cloud server developed by the Google Research team. \\\n",
    "\\\n",
    "The use of Colab is very simple, very similar to the operation of jupyter notebook, you can find the tutorials in the following links:  \\\n",
    "https://www.youtube.com/watch?v=inN8seMm7UI     \\\n",
    "https://research.google.com/colaboratory/faq.html   \\\n",
    "https://towardsdatascience.com/getting-started-with-google-colab-f2fff97f594c  \n",
    "\\\n",
    "I have uploaded the data and code related to travel demand forecasting in my Google drive, \\\n",
    "you can directly click the link below to run the code directly on the cloud through the Colab file I shared. \\\n",
    "There is no need to configure any environment, but you need to have a Google account.   \\\n",
    "https://drive.google.com/file/d/1l0ABIRcz9ygTtaZJ2wVAgPrKq-Mvso3-/view?usp=sharing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425498b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "c1ee295002605422d65ec4aa6ec42d6a996423078b097c1d66cce6e5a5c32798"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
