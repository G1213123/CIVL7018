{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a)\n",
    "\n",
    "The PCA is conduct by obtaining new parameters $z$ by mapping them with the old paramters $x$ in a linear relation.\n",
    "\n",
    "The correlation of $z$ and $x$ is linked by a loading weight $w$, saying how much weighting of each old parameters is considred by the new parameter $z$.\n",
    "\n",
    "$$\n",
    "z_1 = w_1^Tx\n",
    "$$\n",
    "\n",
    "To find the new $z$, we try to maximize the variance of the new $z$, such that it can capture the most information from the old $x$ into the new $z$, hence reducing the dimension of the paramter space.\n",
    "\n",
    "$$\n",
    "Var(z_1) = Var(w_1^Tx)\n",
    "$$\n",
    "$$\n",
    "= E((w_1^Tx-w_1^T\\mu)^2)\n",
    "$$\n",
    "$$\n",
    "= w_1^TE((x-\\mu)(x-\\mu)^T)w_1\n",
    "$$\n",
    "$$\n",
    "= w_1^T\\Sigma w_1\n",
    "$$\n",
    "\n",
    "Since we try the maximize the loading weight, it has to be set on a constraint to prevent the weights bloating into excessive value. We try to limit the weights by noramlizing its distance into 1:\n",
    "$$\n",
    "||w_1|| = \\sum_{n}{w_{n1}^2} = 1\n",
    "$$\n",
    "\n",
    "This singly constrainted linear programming can be solved by differentiating the Lagrangian equation of the model with respect to the $w_1$\n",
    "$$\n",
    "\\mathcal{L}(w_1) = Var(z_1) - \\alpha (||w_1|| - 1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= Var(w_1^Tx) - \\alpha (||w_1|| - 1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= w_1^T\\Sigma w_1 - \\alpha (w_1^Tw_1 - 1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}(w_1)}{\\partial{w_1}} = \\Sigma w_1 - \\alpha w_1 = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "(\\Sigma  - \\alpha I)w_1 = 0\n",
    "$$\n",
    "\n",
    "This implies $\\alpha$, as a scalar variable, should be the eigenvalue and $w_1$ should be a eigenvector for $\\Sigma$, a nxn matrix where n is the dimension of $x$, to be reduced into the formula.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b)\n",
    "\n",
    "- PCA can lower the dimension of the variable vector, hence saving the calculation times and resources for the model prediction.\n",
    "- It can reduce the variable counts, hence identify the most critical variables out of the provided.\n",
    "- By looking at the loading vector of the variables to the principal, one can identify the collinearity of the variables if the loading vectors are similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c)\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "x & y & z\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "-1 & -1 & 2 \\\\\n",
    "-2 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mu_x = \\frac{\\sum_{n}{x_n}}{n} = \\frac{-1-2}{2} = -1.5\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mu_y = \\frac{\\sum_{n}{x_n}}{n} = \\frac{-1+1}{2} = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mu_z = \\frac{\\sum_{n}{x_n}}{n} = \\frac{2+1}{2} = 1.5\n",
    "$$\n",
    "\n",
    "Calculate the Covariance\n",
    "$$\n",
    "\\sum = \\frac{1}{N} \\sum_{n}{(x_n-\\mu)(x_n-\\mu)^T}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\begin{bmatrix}\n",
    "var(x) & cov(x,y) & cov(x,z) \\\\\n",
    "cov(y,x) & var(y) & cov(y,z) \\\\\n",
    "cov(z,x) & cov(z,y) & var(z) \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "=  \\begin{bmatrix}\n",
    "\\frac{(-1+1.5)^2+(-2+1.5)^2}{2} & \\frac{(-1+1.5)(-1-0)+(-2+1.5)(1-0)}{2} & \\frac{(-1+1.5)(2-1.5)+(-2+1.5)(1-1.5)}{2} \\\\\n",
    "\\frac{(-1-0)(-1+1.5)+(1-0)(-2+1.5)}{2} & \\frac{(-1-0)^2+(1-0)^2}{2} & \\frac{(-1-0)(2-1.5)+(1-0)(1-1.5)}{2} \\\\\n",
    "\\frac{(2-1.5)(-1+1.5)+(1-1.5)(-2+1.5)}{2} & \\frac{(2-1.5)(-1-0)+(1-1.5)(1-0)}{2} & \\frac{(2-1.5)^2+(1-1.5)^2}{2} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\begin{bmatrix}\n",
    "0.25 & -0.5 & 0.25\\\\\n",
    "-0.5 & 1 & -0.5\\\\\n",
    "0.25 & -0.5 & 0.25\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The derivitive of the Lagrangian will yield the follwoing equation with the eigenvalue to be solved:\n",
    "\n",
    "$$\n",
    "\\Sigma w = \\alpha w\n",
    "$$\n",
    "\n",
    "Rewrite the eigen equation:\n",
    "\n",
    "$$\n",
    "(\\Sigma - \\alpha I)w =0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{vmatrix}\n",
    "0.25 - \\alpha & -0.5 & 0.25\\\\\n",
    "-0.5 & 1 - \\alpha & -0.5\\\\\n",
    "0.25 & -0.5 & 0.25 - \\alpha \\\\\n",
    "\\end{vmatrix} = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "= (0.25 - \\alpha)\\begin{vmatrix} 1 - \\alpha  & -0.5 \\\\ -0.5 & 0.25 - \\alpha \\end{vmatrix} \n",
    "   - (-0.5) \\begin{vmatrix} -0.5 & -0.5 \\\\ 0.25 & 0.25 - \\alpha \\end{vmatrix} \n",
    "   +  0.25 \\begin{vmatrix} -0.5 & 1 - \\alpha \\\\ 0.25 & -0.5 \\end{vmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "(0.25 - \\alpha)(\\alpha^2 -1.25\\alpha + 0.25 - 0.25) + 0.5 (-0.125 + 0.5\\alpha + 0.125 ) + 0.25(0.25 - 0.25 + 0.25\\alpha)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= -\\alpha^3 + 1.5\\alpha^2 - 0.3125\\alpha  + 0.25 \\alpha + 0.0625 \\alpha\n",
    "$$\n",
    "\n",
    "$$\n",
    "= -\\alpha^3 + 1.5\\alpha^2 \n",
    "$$\n",
    "\n",
    "$$\n",
    "= -\\alpha^2(\\alpha - 1.5)\n",
    "$$\n",
    "\n",
    "$\\alpha = 1.5$ or $\\alpha = 0$\n",
    "\n",
    "Choosing the largest eigenvalue $\\alpha = 1.5$ for maximizing the variance\n",
    "\n",
    "Substitube back to the eigen equation\n",
    "\n",
    "$$\n",
    "(\\Sigma - \\alpha I)w = \n",
    "(\\begin{bmatrix}\n",
    "0.25  & -0.5 & 0.25\\\\\n",
    "-0.5 & 1 & -0.5\\\\\n",
    "0.25 & -0.5 & 0.25\\\\\n",
    "\\end{bmatrix}\n",
    "-\n",
    "\\begin{bmatrix}\n",
    "1.5  & 0 & 0\\\\\n",
    "0 & 1.5 & 0\\\\\n",
    "0 & 0 & 1.5 \\\\\n",
    "\\end{bmatrix})w\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "-1.25w_1  -0.5w_2 + 0.25w_3\\\\\n",
    "-0.5w_1 -0.5w_2 + -0.5w_3\\\\\n",
    "0.25w_1 -0.5w_2 - 1.25w_3\\\\\n",
    "\\end{bmatrix}\n",
    "=0\n",
    "$$\n",
    "\n",
    "Assume $w_1 = k$, from the eigenequations\n",
    "$$\n",
    "w_2 = -2k, \\\n",
    "w_3 = k\n",
    "$$\n",
    "\n",
    "$$\n",
    "w = k \\begin{bmatrix} 1\\\\\n",
    "-2\\\\\n",
    "1\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Normalize the vector s.t. $||w||=1$\n",
    "$$\n",
    "||w|| = k \\sqrt{1^2 + (-2)^2 + 1^2} = 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "k = \\frac{1}{\\sqrt{6}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "w = \\begin{bmatrix} \\frac{1}{\\sqrt{6}}\\\\\n",
    "-\\frac{2}{\\sqrt{6}}\\\\\n",
    "\\frac{1}{\\sqrt{6}}\\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
