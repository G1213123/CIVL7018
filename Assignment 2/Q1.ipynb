{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)\n",
    "## (i)ReLU\n",
    "ReLU activation problem may alleviate the vanishing problem since it has no upper limit for its positive side of function, but the problem may still exist since when the input has reach 0, which the output will also be 0, causing the update weight in gradient descent method to be unable to update the new value for propagation.\n",
    "\n",
    "## (ii)tanh\n",
    "Tanh will cause the vanishing problem since the gradient of the function is in the range of [-1,1]. When there are multiple hidden layer in the MLP, the backprogergation alogrithm will update the weight of the input layer from the errors in output layer by chain ruling the multiple layers' gradient. This will result in the multification of various layer gradient that are all less than 1, diminishing the weight to be update and may stagnate the update rule.\n",
    "\n",
    "## (iii)Leaky ReLU\n",
    "Leaky ReLU is a modified ReLU function than has altered the negative side of ReLU. The negative side of Leaky ReLU is a hyperparameter $\\alpha$, usually very small as <0.0001, multiple with the input x. This is to resolve the vanishing problem of ReLU which is seen when its input reach 0 or negative, which will result in a 0 for gradient descent update rule. THe Leaky ReLu will not lead to the vanishing problem is both positive and negive side. It may still has the vanishing problem if the input give in 0, causing the gradient to be non-computable, but this should rarely happens if the dataset is thoroughly validated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)\n",
    "$L_1$ Regularization, also known as Lasso, will have a weight spacity, which will select the important parameters with a non-zero weight while others being insignificant and having a zero weight.\n",
    "This is due to the error fuction additional term being introduced when adopting the regularization term.\n",
    "For $L_1$, the loss function is added with the weight term multiply to the parameters vector\n",
    "$$E(w) =  E(w) + \\lambda |w| $$\n",
    "\n",
    "For $L_2$, the loss function is added with the weight term multiply to the square of parameters vector\n",
    "$$E(w) =  E(w) + \\lambda ||w||^2 $$\n",
    "\n",
    "When applying backpropagation, the derivative of $L_1$ added term will be in simple $\\lambda$, which can be solved as close form soultion for the new update weight. Hence weight sparscity can be achieve when solving the weight parameters, pushing those insignificant parameter into 0.\n",
    "\n",
    "However for $L_2$, since the derivative of the added term constains 2 times the parameters, the 0 weight cannot be achieved and the insignificant parameters would still have a small weight to the model output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)\n",
    "\n",
    "Key Components of an LSTM:\n",
    "\n",
    "Memory Cell (Cell State): The memory cell stores information over time. It can selectively update or forget information based on input and gate signals.\n",
    "\n",
    "Input Gate: Determines how much new information should be added to the memory cell.\n",
    "\n",
    "Forget Gate: Controls which information from the previous state should be discarded.\n",
    "\n",
    "Output Gate: Determines the output of the LSTM based on the current input and memory cell state.\n",
    "\n",
    "\n",
    "Benefits of LSTMs:\n",
    "\n",
    "Long-Term Dependencies: LSTMs excel at capturing long-range dependencies, making them suitable for tasks like language modeling and music generation.\n",
    "\n",
    "Robustness to Noise: Their gated architecture allows LSTMs to handle noisy input data effectively.\n",
    "\n",
    "Contextual Information: LSTMs consider context information, enabling them to make informed predictions based on past inputs.\n",
    "\n",
    "Applications: LSTMs are widely used in natural language processing, speech recognition, and time series analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d)\n",
    "Training techniques of ChatGPT mainly include 3 steps.\n",
    "1. Pre-training\n",
    "2. Supervised fine-tuning or instruction tuning\n",
    "3. Reinforcement Learning from Human Feedback (RLHF)\n",
    "\n",
    "Pre-training is done using the gather internet public availble large scale data of text to train the model in predicting the sucessor word given the previous passage.\n",
    "\n",
    "Supervised fine-tuning or instruction tuning is to adapt the GPT model with the awareness of the prompt meaning. This is done by training another model with pre-labelled instruction and response to unify the model's answer toward the input prompt's instruction.\n",
    "\n",
    "Reinforcement Learning from Human Feedback (RLHF) is the final safeguard to the GPT's response by rating their response from worse to best from human users. This can allow the GPT model to adapt for a wider variety of situation beyond the labelled dataset and understand real human preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e)\n",
    "\n",
    "for transport engineering domain, LLM can be use to sumamrise all relavant design mannual and codes when conducting transport study or assessment. This can help engineers to quickly pinpoint the requirements and context that they requires to perform assessment and design, through the LLM model that vectorized the design guidelines and compares with the engineers input prompt for searching the related materials in the guidelines. the LLM can also explains the responses with a lament terms for apprentices to understand and speed up their works hour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f)\n",
    "\n",
    "The first method  is to introduce the penalty term to the loss function, i.e. apply $L_1$ regularization. This can reduce the risk of overfitting by removing the redundant parameters that may affect the prediction for dataset that is not within the training dataset and contains parameters that is in abnormal range.\n",
    "\n",
    "Second method is to dropout the hidden layer unit of input by  a p% prabability temporarily. This can help spread out the weighting of parameters into a more wide spread networks node during the trainnag process, without depending highly toward one of few units of inputs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g)\n",
    "\n",
    "The exploration and exploitation is to antagonistic strategy in reinforced learning with similar concept to trial-and-error. In the beginning of model training, the environment that govern the actions to be taken in the model next step is unknown to the model, same for the consequence of the action which is also unknown. After a few trials of the action, the models has gathered information of its action as well as the envrionment. The remaining decision of the model will focus on either exploring the environment which may yield better information for it to make a better decision, or organize its past trials information to come up a better decision. Either move will exempt the other policy in the same move, thus its requires a $\\epsilon$ rate to govern the model to decision which move to make during each decision."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
