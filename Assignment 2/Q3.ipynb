{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Layer   | Activation map dimensions  | Number of parameters  |\n",
    "|---|---|---|\n",
    "|INPUT| 256 × 256 × 3   | 0  |\n",
    "|CONV-3-32   | 254 x 254 x 32  | 896  |\n",
    "|POOL-2   | 127 x 127 x 32 | 0 |\n",
    "|CONV-5-64   | 123 x 123 x 64  | 51264  |\n",
    "|POOL-2   |  61 x 61 x 64 | 0  |\n",
    "|FC-3   | 1 x 1 x 3  | 714435  |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "\n",
    "\n",
    "#here in the snippet below\n",
    "#D = 5 (first parameter)\n",
    "#Stride= (1,1) by default\n",
    "model.add(keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(256, 256, 3)))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(keras.layers.Conv2D(64, (5,5), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(keras.layers.Dense(3, activation='relu'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLP model can contains the layers of input layer, hidden layers and output layers.\n",
    "Each neurons in the layers is connected to he neurons in the previous layer by an activation function taking in the previous neuron value, the weights for each neuron and a bias.\n",
    "The true label value of the output layer can then be used to backward propagate the weight and bias of the model to obtain a valid model for predicting the ouptut value.\n",
    "\n",
    "\n",
    "assuming the input is still as 256 x 256 x 3 in dimension, and for simplicity of the model only 1 hidden layer in the model\n",
    "\n",
    "each node in the input layer will be connect to  a hidden layer of dimension H, then to the output layer with dimension 3 (3 nodes as in problem)\n",
    "\n",
    "The connection between the input and hidden layer will have parameters (assuming there is a bias):\n",
    "\n",
    "$$\n",
    "(256 \\times 256 \\times 3 + 1) \\times H = 196609H\n",
    "$$\n",
    "\n",
    "When connecting to the final output layer, the total parameter count of the model will be\n",
    "\n",
    "$$\n",
    "(196609H + 1 ) \\times 3\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can see that the MLP model ususally contains parameters count of H times to the CNN models, give the hidden layer dimension of H. \n",
    "\n",
    "The clear benefits of CNN models is that there is much less parameters to be estimated during training compares of MLP, which can lower the computation power and time needed for developing a model.\n",
    "\n",
    "CNN also has another benefits that it can take in the relation of pixel in an image, since it harness the convolution of a tensor of image pixel to predict the required activation value for the next layer. On the other hand MLP hidden take in the whole image as vector input with no discrimation is accouting for the postion of the image pixel location. This allow CNN to better extract object features from a large image and can make classification base on part of the features in the picture that is valid, while MLP can only do classification base on the whole image."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
