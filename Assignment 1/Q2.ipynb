{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 2.1.1\n",
    "\n",
    "From dataset the dependent variable of the samples $y_n$ is the total in flow for each station\n",
    "$$\n",
    "Y = \\begin{bmatrix}\n",
    "3459623&3914019&8100630&13460142&2535732\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "the independent variable of the samples $x_n$ consists of 4-dimensional data& which extends its value in the axis of Total Population near each station ($x_{n1}$) number of households that own 0 vehicles ($x_{n2}$) total employment ($x_{n3}$) and total road network density ($x_{n4}$). In each axis& the $x_{nj}$ is a column vector containing the data value of each sample in the dataset.\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "x_{n1}^T & x_{n2}^T & x_{n3}^T & x_{n4}^T\n",
    "\\end{bmatrix}\t\n",
    "$$\n",
    "$$\n",
    "= \\begin{bmatrix}\n",
    "50383&4784&28318&28.7\\\\\n",
    "11084&1664&33120&42.23\\\\\n",
    "51122&16059&61815&36.3\\\\\n",
    "25970&5383&181995&40.15\\\\\n",
    "29222&2891&23981&31.3\n",
    "\\end{bmatrix}\t\n",
    "$$\n",
    "\n",
    "Adding a column vector of 1 to the independent variable matrix to introduct an intercept to the model\n",
    "$$\n",
    "X= \\begin{bmatrix}\n",
    "1&50383&4784&28318&28.7\\\\\n",
    "1&11084&1664&33120&42.23\\\\\n",
    "1&51122&16059&61815&36.3\\\\\n",
    "1&25970&5383&181995&40.15\\\\\n",
    "1&29222&2891&23981&31.3\n",
    "\\end{bmatrix}\t\n",
    "$$\n",
    "\n",
    "Since the model is built as a linear regression model, there is the coefficient matrix $w$, which by taking dot product to the independent variable will give a close proximation to the dependent variable\n",
    "$$\n",
    "y_n \\approx w^Tx_n\n",
    "$$\n",
    "\n",
    "We obtains the close proximation of the model by maximum likeihood estimate, i.e. minimizing the error function\n",
    "$$\n",
    "E(w) = \\frac{1}{2}\\sum_{n=1}^{N}{[y_n-w^Tx_n]^2}\n",
    "$$\n",
    "\n",
    "Using the close form solution formula (25) for lecture 2, obtained by taking derivitive to the error function and setting it equal 0\n",
    "\n",
    "$$\n",
    "w = (X^TX)^{-1}X^TY\n",
    "$$\n",
    "$$\n",
    "=[5288434.545,39.269,127.973,59.180,156149.881]^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2.1.2\n",
    "Declare the objective matrix and the sample matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "X1 = np.array([[1,50383,4784,28318,28.7],[1,11084,1664,33120,42.23],\n",
    "               [1,51122,16059,61815,36.3],[1,25970,5383,181995,40.15],\n",
    "               [1,29222,2891,23981,31.3]])\n",
    "Y1 = np.array([3459623,3914019,8100630,13460142,2535732]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving the MLE with the matrix product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The w matrix with 5 variables [w0, w1, w2, w3, w4] is  [-5288434.54549981       39.26866197      127.97287239       59.18005265\n",
      "   156149.88142737]\n"
     ]
    }
   ],
   "source": [
    "g1 = X1.T@X1\n",
    "g2 = np.linalg.inv(g1)\n",
    "g3 = X1.T @ Y1\n",
    "w = np.linalg.inv(X1.T@X1)@X1.T @ Y1\n",
    "print(\"The w matrix with 5 variables [w0, w1, w2, w3, w4] is \" , w.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving the MLE with the sklearn regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[w1, w2, w3, w4] is [    39.26866197    127.97287239     59.18005265 156149.88142737]\n",
      "w0 is -5288434.545499415\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X2 = np.array([[50383,4784,28318,28.7],[11084,1664,33120,42.23],\n",
    "               [51122,16059,61815,36.3],[25970,5383,181995,40.15],\n",
    "               [29222,2891,23981,31.3]])\n",
    "Y2 = np.array([3459623,3914019,8100630,13460142,2535732])\n",
    "\n",
    "reg = LinearRegression().fit(X2, Y2)\n",
    "\n",
    "print('[w1, w2, w3, w4] is', reg.coef_)\n",
    "print('w0 is', reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated rideship inflow at Montgomery is 12136091.00253777\n"
     ]
    }
   ],
   "source": [
    "X3 = np.array([1, 34689, 9443, 148355, 38.9])\n",
    "Y3 = w.T@X3\n",
    "print('estimated rideship inflow at Montgomery is', Y3[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2.2.1\n",
    "True\n",
    "# Q2.2.2\n",
    "True\n",
    "# Q2.2.3\n",
    "from definition, $\\hat{y} = Xw$\n",
    "\n",
    "the difference vector of $\\hat{y}$ and $y$ = $y - \\hat{y} = y - Xw$\n",
    "\n",
    "Since $y - \\hat{y}$ is orthogonal to $X$, \n",
    "$$\n",
    "X^T(y - \\hat{y}) = 0\\\\\n",
    "X^T(y - Xw) = 0\\\\\n",
    "X^Ty - X^TXw = 0\\\\\n",
    "X^TXw = X^Ty \\\\\n",
    "w = (X^TX)^{-1}X^Ty\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
